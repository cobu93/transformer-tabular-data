{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_utils\n",
    "import builders\n",
    "import importlib\n",
    "\n",
    "from ray import tune\n",
    "import optuna\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "import inspect\n",
    "import argparse\n",
    "\n",
    "import skorch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "\n",
    "from sklearn import base, pipeline, preprocessing, compose, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = \"adult\"\n",
    "aggregator_str = \"cls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULE = f\"{dataset}.{aggregator_str}.config\"\n",
    "CHECKPOINT_DIR = f\"./{dataset}/{aggregator_str}/checkpoint\"\n",
    "SEED = 11\n",
    "N_SAMPLES = 30\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 500 \n",
    "EARLY_STOPPING = 15\n",
    "MAX_CHECKPOINTS = 10\n",
    "multiclass = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_type(module, class_type):\n",
    "    for attr in dir(module):\n",
    "        clazz = getattr(module, attr)\n",
    "        if callable(clazz) and inspect.isclass(clazz) and issubclass(clazz, class_type) and not str(clazz)==str(class_type):\n",
    "            return clazz\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_startswith(params, prefix):\n",
    "    keys = [k for k in params.keys() if k.startswith(prefix)]\n",
    "    extracted = {}\n",
    "\n",
    "    for k in keys:\n",
    "        extracted[k.replace(prefix, \"\")] = params.pop(k)\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = importlib.import_module(MODULE)\n",
    "\n",
    "dataset = get_class_from_type(module, builders.DatasetConfig)\n",
    "if dataset is not None:\n",
    "    dataset = dataset()\n",
    "else:\n",
    "    raise ValueError(\"Dataset configuration not found\")\n",
    "\n",
    "transformer_config = get_class_from_type(module, builders.TransformerConfig)\n",
    "if transformer_config is not None:\n",
    "    transformer_config = transformer_config()\n",
    "else:\n",
    "    raise ValueError(\"Transformer configuration not found\")\n",
    "\n",
    "search_space_config = get_class_from_type(module, builders.SearchSpaceConfig)\n",
    "if search_space_config is not None:\n",
    "    search_space_config = search_space_config()\n",
    "else:\n",
    "    raise ValueError(\"Search space configuration not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_preprocessing_pipeline(categorical_cols, numerical_cols):\n",
    "    categorical_transformer = pipeline.Pipeline(steps=[\n",
    "        ('label', preprocessing.OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = pipeline.Pipeline(steps=[\n",
    "        ('scaler', preprocessing.MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    preprocessing_pipe = pipeline.Pipeline([\n",
    "        ('columns_transformer', compose.ColumnTransformer(\n",
    "            remainder='passthrough', #passthough features not listed\n",
    "            transformers=[\n",
    "                ('categorical_transformer', categorical_transformer , categorical_cols),\n",
    "                ('numerical_transformer', numerical_transformer , numerical_cols)\n",
    "            ]),\n",
    "        ),\n",
    "        ('dtype_transform', nn_utils.DTypeTransformer(np.float32))\n",
    "    ])\n",
    "\n",
    "    return preprocessing_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not dataset.exists():\n",
    "    dataset.download()\n",
    "    \n",
    "dataset.load(seed=None)\n",
    "\n",
    "preprocessor = get_default_preprocessing_pipeline(\n",
    "                        dataset.get_categorical_columns(),\n",
    "                        dataset.get_numerical_columns()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = dataset.get_train_data()\n",
    "val_features, val_labels = dataset.get_val_data()\n",
    "test_features, test_labels = dataset.get_test_data()\n",
    "\n",
    "total_examples = train_features.shape[0] + val_features.shape[0] + test_features.shape[0]\n",
    "\n",
    "print(\"Training examples {} ({})\".format(train_features.shape[0], train_features.shape[0] / total_examples))\n",
    "print(\"Validation examples {} ({})\".format(val_features.shape[0], val_features.shape[0] / total_examples))\n",
    "print(\"Test examples {} ({})\".format(test_features.shape[0], test_features.shape[0] / total_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = preprocessor.fit(train_features, train_labels)\n",
    "\n",
    "train_features = preprocessor.transform(train_features)\n",
    "val_features = preprocessor.transform(val_features)\n",
    "test_features = preprocessor.transform(test_features)\n",
    "\n",
    "all_features, all_labels, indices = nn_utils.join_data([train_features, val_features], [train_labels, val_labels])\n",
    "train_indices, val_indices = indices[0], indices[1]\n",
    "\n",
    "if dataset.get_n_labels() <= 2:\n",
    "    n_labels = 1\n",
    "    criterion = torch.nn.BCEWithLogitsLoss\n",
    "else:\n",
    "    n_labels = dataset.get_n_labels()\n",
    "    multiclass = True\n",
    "    criterion = torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(\n",
    "    train_indices,\n",
    "    validation_indices,\n",
    "    callbacks,\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_head, # Number of heads per layer\n",
    "    n_hid, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_output, # The number of output neurons\n",
    "    embed_dim,\n",
    "    dropout=0.1, # Used dropout\n",
    "    aggregator=None, # The aggregator for output vectors before decoder\n",
    "    categorical_preprocessor=None,\n",
    "    numerical_preprocessor=None,\n",
    "    need_weights=False,\n",
    "    numerical_passthrough=False,\n",
    "    decoder_hidden_units=None,\n",
    "    decoder_activation_fn=None,\n",
    "    **kwargs\n",
    "    ):\n",
    "\n",
    "    module = TabTransformer(\n",
    "        categories = (8, 16, 7, 14, 6, 5, 2, 41),      # tuple containing the number of unique values within each category\n",
    "        num_continuous = 6,                # number of continuous values\n",
    "        dim = 32,                           # dimension, paper set at 32\n",
    "        dim_out = 1,                        # binary prediction, but could be anything\n",
    "        depth = 6,                          # depth, paper recommended 6\n",
    "        heads = 8,                          # heads, paper recommends 8\n",
    "        attn_dropout = 0.1,                 # post-attention dropout\n",
    "        ff_dropout = 0.1,                   # feed forward dropout\n",
    "        mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "        mlp_act = torch.nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # Define model\n",
    "    module = TabularTransformer(\n",
    "        n_head, # Number of heads per layer\n",
    "        n_hid, # Size of the MLP inside each transformer encoder layer\n",
    "        n_layers, # Number of transformer encoder layers    \n",
    "        n_output, # The number of output neurons\n",
    "        torch.nn.ModuleList(encoders), # List of features encoders\n",
    "        dropout=dropout, # Used dropout\n",
    "        aggregator=aggregator, # The aggregator for output vectors before decoder\n",
    "        preprocessor=preprocessor,\n",
    "        need_weights=need_weights,\n",
    "        numerical_passthrough=numerical_passthrough\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    model = skorch.NeuralNetClassifier(\n",
    "            module=module,\n",
    "            train_split=skorch.dataset.CVSplit(((train_indices, validation_indices),)),\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model_ft(\n",
    "    train_indices,\n",
    "    validation_indices,\n",
    "    callbacks,\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_head, # Number of heads per layer\n",
    "    n_hid, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_output, # The number of output neurons\n",
    "    embed_dim,\n",
    "    dropout=0.1, # Used dropout\n",
    "    aggregator=None, # The aggregator for output vectors before decoder\n",
    "    categorical_preprocessor=None,\n",
    "    numerical_preprocessor=None,\n",
    "    need_weights=False,\n",
    "    numerical_passthrough=False,\n",
    "    decoder_hidden_units=None,\n",
    "    decoder_activation_fn=None,\n",
    "    **kwargs\n",
    "    ):\n",
    "\n",
    "    module = FTTransformer(\n",
    "        categories = (8, 16, 7, 14, 6, 5, 2, 41),      # tuple containing the number of unique values within each category\n",
    "        num_continuous = 6,                # number of continuous values\n",
    "        dim = 32,                           # dimension, paper set at 32\n",
    "        dim_out = 1,                        # binary prediction, but could be anything\n",
    "        depth = 6,                          # depth, paper recommended 6\n",
    "        heads = 8,                          # heads, paper recommends 8\n",
    "        attn_dropout = 0.1,                 # post-attention dropout\n",
    "        ff_dropout = 0.1,                   # feed forward dropout\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # Define model\n",
    "    module = TabularTransformer(\n",
    "        n_head, # Number of heads per layer\n",
    "        n_hid, # Size of the MLP inside each transformer encoder layer\n",
    "        n_layers, # Number of transformer encoder layers    \n",
    "        n_output, # The number of output neurons\n",
    "        torch.nn.ModuleList(encoders), # List of features encoders\n",
    "        dropout=dropout, # Used dropout\n",
    "        aggregator=aggregator, # The aggregator for output vectors before decoder\n",
    "        preprocessor=preprocessor,\n",
    "        need_weights=need_weights,\n",
    "        numerical_passthrough=numerical_passthrough\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    model = skorch.NeuralNetClassifier(\n",
    "            module=module,\n",
    "            train_split=skorch.dataset.CVSplit(((train_indices, validation_indices),)),\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"n_layers\": 6,\n",
    "            \"optimizer__lr\": 10e-4,#tune.choice([10e-6, 10e-5, 10e-4, 10e-3]),\n",
    "            \"optimizer__weight_decay\": 10e-1,\n",
    "            \"n_head\": 8, # Number of heads per layer\n",
    "            \"n_hid\": 128, # Size of the MLP inside each transformer encoder layer\n",
    "            \"dropout\": 0.1, #tune.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5]), # Used dropout\n",
    "            \"embedding_size\": 32,\n",
    "            \"numerical_passthrough\": False\n",
    "        }\n",
    "\n",
    "\n",
    "embedding_size = config.pop(\"embedding_size\")\n",
    "\n",
    "encoders_params = get_params_startswith(config, \"encoders__\")\n",
    "aggregator_params = get_params_startswith(config, \"aggregator__\")\n",
    "preprocessor_params = get_params_startswith(config, \"preprocessor__\")\n",
    "\n",
    "model_params = {\n",
    "    **config,\n",
    "    #\"encoders\": transformer_config.get_encoders(embedding_size, **{**config, **encoders_params}),\n",
    "    \"n_categories\": (8, 16, 7, 14, 6, 5, 2, 41),\n",
    "    \"n_numerical\": 6,\n",
    "    \"embed_dim\": 32,\n",
    "    \"aggregator\": transformer_config.get_aggregator(embedding_size, **{**config, **aggregator_params}),\n",
    "    \"numerical_preprocessor\": transformer_config.get_preprocessor(**{**config, **preprocessor_params}),\n",
    "    \"optimizer\": torch.optim.AdamW,\n",
    "    \"criterion\": criterion,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"max_epochs\": MAX_EPOCHS,\n",
    "    \"n_output\": n_labels, # The number of output neurons\n",
    "    \"need_weights\": False,\n",
    "    \"decoder_hidden_units\": [128, 64],\n",
    "    \"decoder_activation_fn\": nn.ReLU(),\n",
    "    \"verbose\": 1\n",
    "    \n",
    "}\n",
    "\n",
    "model = nn_utils.build_transformer_model(\n",
    "            train_indices,\n",
    "            val_indices,\n",
    "            nn_utils.get_default_callbacks(seed=SEED, multiclass=multiclass),\n",
    "            **model_params\n",
    "            )\n",
    "\n",
    "#model = build_transformer_model_ft(\n",
    "#            train_indices,\n",
    "#            val_indices,\n",
    "#            nn_utils.get_default_callbacks(seed=SEED, multiclass=multiclass),\n",
    "#            **model_params\n",
    "#            )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_categorical\": all_features[:, :8].astype(np.int32), \n",
    "    \"x_numerical\": all_features[:, 8:].astype(np.float32)\n",
    "    }, \n",
    "    y=all_labels)\n",
    "\n",
    "\n",
    "# TabTransformer\n",
    "#model = model.fit(X={\n",
    "#    \"x_categ\": all_features[:, :8].astype(np.int32), \n",
    "#    \"x_cont\": all_features[:, 8:].astype(np.float32)\n",
    "#    }, \n",
    "#    y=all_labels[:, np.newaxis].astype(np.double))\n",
    "\n",
    "# FT-Transformer \n",
    "model = model.fit(X={\n",
    "    \"x_categ\": all_features[:, :8].astype(np.int32), \n",
    "    \"x_numer\": all_features[:, 8:].astype(np.float32)\n",
    "    }, \n",
    "    y=all_labels[:, np.newaxis].astype(np.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config, checkpoint_dir=CHECKPOINT_DIR):\n",
    "    \n",
    "    embedding_size = config.pop(\"embedding_size\")\n",
    "\n",
    "    encoders_params = get_params_startswith(config, \"encoders__\")\n",
    "    aggregator_params = get_params_startswith(config, \"aggregator__\")\n",
    "    preprocessor_params = get_params_startswith(config, \"preprocessor__\")\n",
    "\n",
    "    model_params = {\n",
    "        **config,\n",
    "        \"encoders\": transformer_config.get_encoders(embedding_size, **{**config, **encoders_params}),\n",
    "        \"aggregator\": transformer_config.get_aggregator(embedding_size, **{**config, **aggregator_params}),\n",
    "        \"preprocessor\": transformer_config.get_preprocessor(**{**config, **preprocessor_params}),\n",
    "        \"optimizer\": torch.optim.AdamW,\n",
    "        \"criterion\": criterion,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"n_output\": n_labels, # The number of output neurons\n",
    "        \"need_weights\": False,\n",
    "        \"verbose\": 1\n",
    "        \n",
    "    }\n",
    "    \n",
    "    model = nn_utils.build_transformer_model(\n",
    "                train_indices,\n",
    "                val_indices,\n",
    "                nn_utils.get_default_callbacks(seed=SEED, multiclass=multiclass),\n",
    "                **model_params\n",
    "                )\n",
    "    \n",
    "    model = model.fit(X=all_features, y=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "            \"n_layers\": tune.choice([6]), # Number of transformer encoder layers    \n",
    "            \"optimizer__lr\": tune.choice([10e-6, 10e-5, 10e-4, 10e-3]),\n",
    "            \"n_head\": tune.choice([8]), # Number of heads per layer\n",
    "            \"n_hid\": tune.choice([56]), # Size of the MLP inside each transformer encoder layer\n",
    "            \"dropout\": tune.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5]), # Used dropout\n",
    "            \"embedding_size\": tune.choice([32]),\n",
    "            \"numerical_passthrough\": tune.choice([True])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_modes = [\"AUTO\", \"ERRORED_ONLY\"]\n",
    "\n",
    "\n",
    "for try_cnt, resume_mode in enumerate(resume_modes):\n",
    "    try:\n",
    "        0 / 0\n",
    "\n",
    "        analysis = tune.run(\n",
    "            trainable,\n",
    "            config=search_space,\n",
    "            resources_per_trial={\n",
    "                \"gpu\": 1,\n",
    "                \"cpu\": 6\n",
    "            },\n",
    "            search_alg=OptunaSearch(\n",
    "                metric=\"roc_auc\",\n",
    "                mode=\"max\",\n",
    "                sampler=optuna.samplers.TPESampler()\n",
    "            ),\n",
    "            num_samples=N_SAMPLES,\n",
    "            fail_fast=True,\n",
    "            checkpoint_score_attr=\"max-roc_auc\",\n",
    "            keep_checkpoints_num=MAX_CHECKPOINTS,\n",
    "            resume=resume_mode,\n",
    "            local_dir=CHECKPOINT_DIR, \n",
    "            name=\"param_search\",\n",
    "            scheduler=AsyncHyperBandScheduler(\n",
    "                            time_attr=\"training_iteration\",\n",
    "                            metric=\"roc_auc\",\n",
    "                            mode=\"max\",\n",
    "                            grace_period=EARLY_STOPPING\n",
    "                        )\n",
    "        )\n",
    "\n",
    "        break\n",
    "    except Exception as e:\n",
    "\n",
    "        if try_cnt + 1 == len(resume_modes):\n",
    "            raise(e)\n",
    "\n",
    "        print(e)\n",
    "        print(\"Retrying in second mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(metric=\"roc_auc\", mode=\"max\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('DCC-attn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "741e1ea5cc84540885a0ba493c428a89b760d4b9c08bcc57bb2b1068c5ec8401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
