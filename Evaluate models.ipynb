{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_utils\n",
    "import builders\n",
    "import importlib\n",
    "\n",
    "from ray import tune\n",
    "import optuna\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "import torch\n",
    "\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.tune import register_trainable\n",
    "\n",
    "import inspect\n",
    "import argparse\n",
    "import skorch\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"sylvine\", \"volkert\",\n",
    "    \"adult\", \"australian\",\n",
    "    \"anneal\",  \n",
    "    \"jasmine\", \"kr_vs_kp\", \n",
    "    \"nomao\", \"ldpa\"\n",
    "]\n",
    "DATASETS = [\n",
    "    \"adult\"\n",
    "]\n",
    "\n",
    "AGGREGATORS = [\"cls\", \"concatenate\", \"rnn\", \"sum\", \"mean\", \"max\"]\n",
    "AGGREGATORS = [\"cls\", \"concatenate\"]\n",
    "BATCH_SIZE = 128\n",
    "SEED = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_configs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, trainable=True):\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.module_.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad and trainable: \n",
    "            continue\n",
    "            \n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "        \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using -- Dataset:adult Aggregator:cls\n",
      "Target mapping: {'<=50K': 0, '>50K': 1}\n",
      "Numerical columns: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Categorical columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "Columns: ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "Not fitted before! I'm not going to do anything\n",
      "Using -- Dataset:adult Aggregator:concatenate\n",
      "Target mapping: {'<=50K': 0, '>50K': 1}\n",
      "Numerical columns: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Categorical columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "Columns: ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n"
     ]
    }
   ],
   "source": [
    "errors= []\n",
    "\n",
    "for dataset_ in DATASETS:\n",
    "    for aggregator_str_ in AGGREGATORS:\n",
    "        \n",
    "        dataset = dataset_\n",
    "        aggregator_str = aggregator_str_\n",
    "       \n",
    "       \n",
    "        print(f\"Using -- Dataset:{dataset} Aggregator:{aggregator_str}\")\n",
    "\n",
    "        #####################################################\n",
    "        # Configuration\n",
    "        #####################################################\n",
    "\n",
    "        MODULE = f\"{dataset}.{aggregator_str}.config\"\n",
    "        CHECKPOINT_DIR = f\"./{dataset}/{aggregator_str}/checkpoint\"\n",
    "        SEED = 11\n",
    "        N_SAMPLES = 30\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "        MAX_EPOCHS = 1000\n",
    "        EARLY_STOPPING = 30\n",
    "        MAX_CHECKPOINTS = 10\n",
    "        multiclass = False\n",
    "\n",
    "        #####################################################\n",
    "        # Util functions\n",
    "        #####################################################\n",
    "\n",
    "        def get_class_from_type(module, class_type):\n",
    "            for attr in dir(module):\n",
    "                clazz = getattr(module, attr)\n",
    "                if callable(clazz) and inspect.isclass(clazz) and issubclass(clazz, class_type) and not str(clazz)==str(class_type):\n",
    "                    return clazz\n",
    "\n",
    "            return None\n",
    "\n",
    "        def get_params_startswith(params, prefix):\n",
    "            keys = [k for k in params.keys() if k.startswith(prefix)]\n",
    "            extracted = {}\n",
    "\n",
    "            for k in keys:\n",
    "                extracted[k.replace(prefix, \"\")] = params.pop(k)\n",
    "\n",
    "            return extracted\n",
    "\n",
    "        def trainable(config, checkpoint_dir=CHECKPOINT_DIR):\n",
    "            embedding_size = config.pop(\"embedding_size\")\n",
    "\n",
    "            aggregator_params = get_params_startswith(config, \"aggregator__\")\n",
    "            preprocessor_params = get_params_startswith(config, \"preprocessor__\")\n",
    "\n",
    "            limit_categories = len( transformer_config.get_n_categories())\n",
    "\n",
    "            model_params = {\n",
    "                **config,\n",
    "                \"n_categories\": transformer_config.get_n_categories(),\n",
    "                \"n_numerical\": transformer_config.get_n_numerical(),\n",
    "                \"embed_dim\": embedding_size,\n",
    "                \"aggregator\": transformer_config.get_aggregator(embedding_size, **{**config, **aggregator_params}),\n",
    "                \"categorical_preprocessor\": transformer_config.get_preprocessor(**{**config, **preprocessor_params}),\n",
    "                \"optimizer\": torch.optim.AdamW,\n",
    "                \"criterion\": criterion,\n",
    "                \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"max_epochs\": MAX_EPOCHS,\n",
    "                \"n_output\": n_labels, # The number of output neurons\n",
    "                \"need_weights\": False,\n",
    "                \"decoder_hidden_units\": transformer_config.get_decoder_hidden_units(),\n",
    "                \"decoder_activation_fn\": transformer_config.get_decoder_activation_fn(),\n",
    "                \"verbose\": 1\n",
    "                \n",
    "            }\n",
    "\n",
    "            if not os.path.exists(os.path.join(CHECKPOINT_DIR, \"best_model/.fitted\")):\n",
    "                print(\"Not fitted before! I'm not going to do anything\")\n",
    "                return\n",
    "\n",
    "\n",
    "            checkpoint = skorch.callbacks.Checkpoint(monitor=\"balanced_accuracy_best\", dirname=os.path.join(CHECKPOINT_DIR, \"best_model\"))\n",
    "\n",
    "            model = nn_utils.build_transformer_model(\n",
    "                        train_indices,\n",
    "                        val_indices, \n",
    "                        [],\n",
    "                        **model_params\n",
    "                        )\n",
    "            model.load_params(checkpoint=checkpoint)\n",
    "            return model\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        #####################################################\n",
    "        # Dataset and components\n",
    "        #####################################################\n",
    "\n",
    "        module = importlib.import_module(MODULE)\n",
    "\n",
    "        dataset = get_class_from_type(module, builders.DatasetConfig)\n",
    "        if dataset is not None:\n",
    "            dataset = dataset()\n",
    "        else:\n",
    "            raise ValueError(\"Dataset configuration not found\")\n",
    "\n",
    "        transformer_config = get_class_from_type(module, builders.TransformerConfig)\n",
    "        if transformer_config is not None:\n",
    "            transformer_config = transformer_config()\n",
    "        else:\n",
    "            raise ValueError(\"Transformer configuration not found\")\n",
    "\n",
    "        search_space_config = get_class_from_type(module, builders.SearchSpaceConfig)\n",
    "        if search_space_config is not None:\n",
    "            search_space_config = search_space_config()\n",
    "        else:\n",
    "            raise ValueError(\"Search space configuration not found\")\n",
    "\n",
    "        #####################################################\n",
    "        # Configure dataset\n",
    "        #####################################################\n",
    "\n",
    "        if not dataset.exists():\n",
    "            dataset.download()\n",
    "\n",
    "        dataset.load(seed=SEED)\n",
    "\n",
    "        preprocessor = nn_utils.get_default_preprocessing_pipeline(\n",
    "                                dataset.get_categorical_columns(),\n",
    "                                dataset.get_numerical_columns()\n",
    "                            )\n",
    "\n",
    "        #####################################################\n",
    "        # Data preparation\n",
    "        #####################################################\n",
    "\n",
    "        train_features, train_labels = dataset.get_train_data()\n",
    "        val_features, val_labels = dataset.get_val_data()\n",
    "        test_features, test_labels = dataset.get_test_data()\n",
    "\n",
    "        preprocessor = preprocessor.fit(train_features, train_labels)\n",
    "\n",
    "        train_features = preprocessor.transform(train_features)\n",
    "        val_features = preprocessor.transform(val_features)\n",
    "        test_features = preprocessor.transform(test_features)\n",
    "\n",
    "        all_features, all_labels, indices = nn_utils.join_data([train_features, val_features], [train_labels, val_labels])\n",
    "        train_indices, val_indices = indices[0], indices[1]\n",
    "\n",
    "        if dataset.get_n_labels() <= 2:\n",
    "            n_labels = 1\n",
    "            criterion = torch.nn.BCEWithLogitsLoss\n",
    "        else:\n",
    "            n_labels = dataset.get_n_labels()\n",
    "            multiclass = True\n",
    "            criterion = torch.nn.CrossEntropyLoss\n",
    "\n",
    "        #####################################################\n",
    "        # Hyperparameter search\n",
    "        #####################################################\n",
    "        \n",
    "        #register_trainable(\"training_function\", training_function)\n",
    "\n",
    "        limit_categories = len( transformer_config.get_n_categories())\n",
    "        \n",
    "        register_trainable(\"trainable\", trainable)\n",
    "        \n",
    "        try:\n",
    "            '''\n",
    "            analysis = tune.run(\n",
    "                trainable,\n",
    "                resume=\"AUTO\",\n",
    "                local_dir=CHECKPOINT_DIR, \n",
    "                name=\"param_search\"    \n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            analysis = ExperimentAnalysis(os.path.join(CHECKPOINT_DIR, \"param_search\"))\n",
    "            \n",
    "            best_config = analysis.get_best_config(metric=\"balanced_accuracy\", mode=\"max\")\n",
    "            '''\n",
    "            if dataset_ not in results:\n",
    "                results[dataset_] = {}\n",
    "\n",
    "            if aggregator_str_ not in results[dataset_]:\n",
    "                results[dataset_][aggregator_str_] = {}\n",
    "                \n",
    "            results[dataset_][aggregator_str_][\"balanced_accuracy\"] = analysis.get_best_trial(metric=\"balanced_accuracy\", mode=\"max\").last_result[\"balanced_accuracy\"]\n",
    "            del analysis\n",
    "            '''\n",
    "            model = trainable(best_config)\n",
    "            y_pred_train = model.predict({\n",
    "                \"x_categorical\": train_features[:, :limit_categories].astype(np.int32), \n",
    "                \"x_numerical\": train_features[:, limit_categories:].astype(np.float32)\n",
    "                })\n",
    "            y_pred_val = model.predict({\n",
    "                \"x_categorical\": val_features[:, :limit_categories].astype(np.int32), \n",
    "                \"x_numerical\": val_features[:, limit_categories:].astype(np.float32)\n",
    "                })\n",
    "            y_pred_test = model.predict({\n",
    "                \"x_categorical\": test_features[:, :limit_categories].astype(np.int32), \n",
    "                \"x_numerical\": test_features[:, limit_categories:].astype(np.float32)\n",
    "                })\n",
    "\n",
    "            if dataset_ not in results:\n",
    "                results[dataset_] = {}\n",
    "                best_configs[dataset_] = {}\n",
    "\n",
    "            if aggregator_str_ not in results[dataset_]:\n",
    "                results[dataset_][aggregator_str_] = {}\n",
    "                best_configs[dataset_][aggregator_str_] = best_config\n",
    "\n",
    "            #results[dataset_][aggregator_str_][\"loss\"] = metrics.log_loss(test_labels, y_pred)\n",
    "            results[dataset_][aggregator_str_][\"balanced_accuracy_train\"] = metrics.balanced_accuracy_score(train_labels, y_pred_train)\n",
    "            results[dataset_][aggregator_str_][\"balanced_accuracy_val\"] = metrics.balanced_accuracy_score(val_labels, y_pred_val)\n",
    "            results[dataset_][aggregator_str_][\"balanced_accuracy_test\"] = metrics.balanced_accuracy_score(test_labels, y_pred_test)\n",
    "            results[dataset_][aggregator_str_][\"n_parameters\"] = count_parameters(model, trainable=False)\n",
    "            results[dataset_][aggregator_str_][\"n_trainable\"] = count_parameters(model)\n",
    "            #results[dataset_][aggregator_str_][\"roc_auc\"] = metrics.roc_auc_score(test_labels, y_pred)\n",
    "            #print(metrics.balanced_accuracy_score(test_labels, y_pred))\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(\"{}.{} - {}\".format(dataset_, aggregator_str_, str(e)))\n",
    "            pass\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adult': {'concatenate': {'balanced_accuracy_train': 0.8022011983862758,\n",
       "   'balanced_accuracy_val': 0.7963661976992864,\n",
       "   'balanced_accuracy_test': 0.79289930085895,\n",
       "   'n_parameters': 354433,\n",
       "   'n_trainable': 354433}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"adult.cls - 'NoneType' object has no attribute 'predict'\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adult': {'concatenate': {'n_layers': 1,\n",
       "   'optimizer__lr': 1.3473254295104352e-05,\n",
       "   'n_head': 16,\n",
       "   'n_hid': 128,\n",
       "   'dropout': 0.014089422184433743,\n",
       "   'numerical_passthrough': False}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_configs.json\", \"w\") as f:\n",
    "    json.dump(best_configs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"adult.cls - 'NoneType' object has no attribute 'predict'\"]\n"
     ]
    }
   ],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adult/concatenate/checkpoint/param_search/searcher-state-2022-04-28_04-26-37.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f47ba86afd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adult/concatenate/checkpoint/param_search/searcher-state-2022-04-28_04-26-37.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adult/concatenate/checkpoint/param_search/searcher-state-2022-04-28_04-26-37.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"adult/concatenate/checkpoint/param_search/searcher-state-2022-04-28_04-26-37.pkl\", \"rb\") as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "obj[0], obj[2]\n",
    "dir(obj[2].best_params)\n",
    "from optuna.visualization import plot_optimization_history\n",
    "plot_optimization_history(obj[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[2].best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "741e1ea5cc84540885a0ba493c428a89b760d4b9c08bcc57bb2b1068c5ec8401"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
