{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_utils\n",
    "import builders\n",
    "import importlib\n",
    "\n",
    "from ray import tune\n",
    "import optuna\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "import torch\n",
    "\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.tune import register_trainable\n",
    "\n",
    "import inspect\n",
    "import argparse\n",
    "import skorch\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"sylvine\", #\"volkert\",\n",
    "    \"adult\", \"australian\",\n",
    "    \"anneal\",  \n",
    "    \"jasmine\", \"kr_vs_kp\", \n",
    "    \"nomao\", \"ldpa\"\n",
    "]\n",
    "AGGREGATORS = [\"cls\", \"concatenate\", \"rnn\", \"sum\", \"mean\", \"max\"]\n",
    "BATCH_SIZE = 128\n",
    "SEED = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_configs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, trainable=True):\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.module.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad and trainable: \n",
    "            continue\n",
    "            \n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "        \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors= []\n",
    "\n",
    "for dataset_ in DATASETS:\n",
    "    for aggregator_str_ in AGGREGATORS:\n",
    "        \n",
    "        dataset = dataset_\n",
    "        aggregator_str = aggregator_str_\n",
    "       \n",
    "       \n",
    "        print(f\"Using -- Dataset:{dataset} Aggregator:{aggregator_str}\")\n",
    "\n",
    "        #####################################################\n",
    "        # Configuration\n",
    "        #####################################################\n",
    "\n",
    "        MODULE = f\"{dataset}.{aggregator_str}.config\"\n",
    "        CHECKPOINT_DIR = f\"./{dataset}/{aggregator_str}/checkpoint\"\n",
    "        SEED = 11\n",
    "        N_SAMPLES = 30\n",
    "        BATCH_SIZE = 128\n",
    "        multiclass = False\n",
    "\n",
    "        #####################################################\n",
    "        # Util functions\n",
    "        #####################################################\n",
    "\n",
    "        def get_class_from_type(module, class_type):\n",
    "            for attr in dir(module):\n",
    "                clazz = getattr(module, attr)\n",
    "                if callable(clazz) and inspect.isclass(clazz) and issubclass(clazz, class_type) and not str(clazz)==str(class_type):\n",
    "                    return clazz\n",
    "\n",
    "            return None\n",
    "\n",
    "        def get_params_startswith(params, prefix):\n",
    "            keys = [k for k in params.keys() if k.startswith(prefix)]\n",
    "            extracted = {}\n",
    "\n",
    "            for k in keys:\n",
    "                extracted[k.replace(prefix, \"\")] = params.pop(k)\n",
    "\n",
    "            return extracted\n",
    "\n",
    "\n",
    "        def trainable(config, checkpoint_dir=CHECKPOINT_DIR):\n",
    "            embedding_size = config.pop(\"embedding_size\")\n",
    "\n",
    "            encoders_params = get_params_startswith(config, \"encoders__\")\n",
    "            aggregator_params = get_params_startswith(config, \"aggregator__\")\n",
    "            preprocessor_params = get_params_startswith(config, \"preprocessor__\")\n",
    "\n",
    "            model_params = {\n",
    "                **config,\n",
    "                \"encoders\": transformer_config.get_encoders(embedding_size, **{**config, **encoders_params}),\n",
    "                \"aggregator\": transformer_config.get_aggregator(embedding_size, **{**config, **aggregator_params}),\n",
    "                \"preprocessor\": transformer_config.get_preprocessor(**{**config, **preprocessor_params}),\n",
    "                \"optimizer\": torch.optim.SGD,\n",
    "                \"criterion\": criterion,\n",
    "                \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"max_epochs\": 1,\n",
    "                \"n_output\": n_labels, # The number of output neurons\n",
    "                \"need_weights\": False,\n",
    "                \"verbose\": 1\n",
    "\n",
    "            }\n",
    "\n",
    "            model = nn_utils.build_transformer_model(\n",
    "                        train_indices,\n",
    "                        val_indices, \n",
    "                        [],\n",
    "                        **model_params\n",
    "                        )\n",
    "            \n",
    "            return model\n",
    "        \n",
    "\n",
    "        #####################################################\n",
    "        # Dataset and components\n",
    "        #####################################################\n",
    "\n",
    "        module = importlib.import_module(MODULE)\n",
    "\n",
    "        dataset = get_class_from_type(module, builders.DatasetConfig)\n",
    "        if dataset is not None:\n",
    "            dataset = dataset()\n",
    "        else:\n",
    "            raise ValueError(\"Dataset configuration not found\")\n",
    "\n",
    "        transformer_config = get_class_from_type(module, builders.TransformerConfig)\n",
    "        if transformer_config is not None:\n",
    "            transformer_config = transformer_config()\n",
    "        else:\n",
    "            raise ValueError(\"Transformer configuration not found\")\n",
    "\n",
    "        search_space_config = get_class_from_type(module, builders.SearchSpaceConfig)\n",
    "        if search_space_config is not None:\n",
    "            search_space_config = search_space_config()\n",
    "        else:\n",
    "            raise ValueError(\"Search space configuration not found\")\n",
    "\n",
    "        #####################################################\n",
    "        # Configure dataset\n",
    "        #####################################################\n",
    "\n",
    "        if not dataset.exists():\n",
    "            dataset.download()\n",
    "\n",
    "        dataset.load(seed=SEED)\n",
    "\n",
    "        preprocessor = nn_utils.get_default_preprocessing_pipeline(\n",
    "                                dataset.get_categorical_columns(),\n",
    "                                dataset.get_numerical_columns()\n",
    "                            )\n",
    "\n",
    "        #####################################################\n",
    "        # Data preparation\n",
    "        #####################################################\n",
    "\n",
    "        train_features, train_labels = dataset.get_train_data()\n",
    "        val_features, val_labels = dataset.get_val_data()\n",
    "        test_features, test_labels = dataset.get_test_data()\n",
    "\n",
    "        preprocessor = preprocessor.fit(train_features, train_labels)\n",
    "\n",
    "        train_features = preprocessor.transform(train_features)\n",
    "        val_features = preprocessor.transform(val_features)\n",
    "        test_features = preprocessor.transform(test_features)\n",
    "\n",
    "        all_features, all_labels, indices = nn_utils.join_data([train_features, val_features], [train_labels, val_labels])\n",
    "        train_indices, val_indices = indices[0], indices[1]\n",
    "\n",
    "        if dataset.get_n_labels() <= 2:\n",
    "            n_labels = 1\n",
    "            criterion = torch.nn.BCEWithLogitsLoss\n",
    "        else:\n",
    "            n_labels = dataset.get_n_labels()\n",
    "            multiclass = True\n",
    "            criterion = torch.nn.CrossEntropyLoss\n",
    "\n",
    "        #####################################################\n",
    "        # Hyperparameter search\n",
    "        #####################################################\n",
    "        \n",
    "        #register_trainable(\"training_function\", training_function)\n",
    "        register_trainable(\"trainable\", trainable)\n",
    "        \n",
    "        try:\n",
    "            '''\n",
    "            analysis = tune.run(\n",
    "                trainable,\n",
    "                resume=\"AUTO\",\n",
    "                local_dir=CHECKPOINT_DIR, \n",
    "                name=\"param_search\"    \n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            analysis = ExperimentAnalysis(os.path.join(CHECKPOINT_DIR, \"param_search\"))\n",
    "            best_config = analysis.get_best_config(metric=\"balanced_accuracy\", mode=\"max\")\n",
    "            \n",
    "            if dataset_ not in results:\n",
    "                results[dataset_] = {}\n",
    "            \n",
    "            if aggregator_str_ not in results[dataset_]:\n",
    "                results[dataset_][aggregator_str_] = {}\n",
    "            \n",
    "            \n",
    "            for trial_idx, trial in enumerate(analysis.trials):\n",
    "                model = trainable(trial.config)\n",
    "                #print(\"*\" * 50)\n",
    "                #print(trial.config)\n",
    "                #print(trial.last_result)\n",
    "                #print(trial.metric_analysis)\n",
    "                #print(trial.checkpoint)\n",
    "                #print(count_parameters(model, trainable=False))\n",
    "                #print(count_parameters(model, trainable=True))\n",
    "                #print(\"*\" * 50)\n",
    "                \n",
    "                results[dataset_][aggregator_str_][trial_idx] = {}\n",
    "                \n",
    "                results[dataset_][aggregator_str_][trial_idx][\"trial\"] = trial_idx\n",
    "                results[dataset_][aggregator_str_][trial_idx][\"config\"] = trial.config\n",
    "                results[dataset_][aggregator_str_][trial_idx][\"balanced_accuracy_max\"] = trial.metric_analysis[\"balanced_accuracy\"][\"max\"]\n",
    "                results[dataset_][aggregator_str_][trial_idx][\"training_iter_sec\"] = trial.metric_analysis[\"time_total_s\"][\"avg\"]\n",
    "                results[dataset_][aggregator_str_][trial_idx][\"non_trainable_params\"] = count_parameters(model, trainable=False)\n",
    "                results[dataset_][aggregator_str_][trial_idx][\"trainable_params\"] = count_parameters(model, trainable=True)\n",
    "            \n",
    "            #model = trainable(best_config)\n",
    "            #y_pred = model.predict(test_features)\n",
    "\n",
    "            #if dataset_ not in results:\n",
    "            #    results[dataset_] = {}\n",
    "            #    best_configs[dataset_] = {}\n",
    "\n",
    "            #if aggregator_str_ not in results[dataset_]:\n",
    "            #    results[dataset_][aggregator_str_] = {}\n",
    "            #    best_configs[dataset_][aggregator_str_] = best_config\n",
    "\n",
    "            #results[dataset_][aggregator_str_][\"loss\"] = metrics.log_loss(test_labels, y_pred)\n",
    "            #results[dataset_][aggregator_str_][\"balanced_accuracy\"] = metrics.balanced_accuracy_score(test_labels, y_pred)\n",
    "            #results[dataset_][aggregator_str_][\"n_parameters\"] = count_parameters(model, trainable=False)\n",
    "            #results[dataset_][aggregator_str_][\"n_trainable\"] = count_parameters(model)\n",
    "            #results[dataset_][aggregator_str_][\"roc_auc\"] = metrics.roc_auc_score(test_labels, y_pred)\n",
    "            #print(metrics.balanced_accuracy_score(test_labels, y_pred))\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(\"{}.{} - {}\".format(dataset_, aggregator_str_, str(e)))\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
